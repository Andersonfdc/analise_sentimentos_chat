import streamlit as st
import os
import re
import gensim
import nltk
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
from keras.models import Sequential
from keras.layers import LSTM, Dense, Embedding, Dropout, Bidirectional
from nltk.sentiment import SentimentIntensityAnalyzer
from gensim.models import Word2Vec
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from langdetect import detect
from deep_translator import GoogleTranslator
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
import pandas as pd
from langdetect import detect
from textblob import TextBlob
from transformers import pipeline, Conversation
from tensorflow.keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Layer
from sklearn.preprocessing import LabelEncoder
from keras.callbacks import Callback
import time
import random
from difflib import SequenceMatcher
import torch

st.set_page_config(
page_title="ChatBot", 
page_icon="	:robot_face:",
)
st.title("An√°lise de sentimentos - chatbot")

nltk.download('punkt', quiet=True)
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('vader_lexicon', quiet=True)
nltk.download('nps_chat', quiet=True)
nltk.download('averaged_perceptron_tagger', quiet=True)

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

sia = SentimentIntensityAnalyzer()
stop_words = set(stopwords.words("portuguese"))

# Inicializa√ß√£o de vari√°veis na sess√£o
if 'model_trained' not in st.session_state:
    st.session_state.model_trained = False
if 'last_sentiment' not in st.session_state:
    st.session_state.last_sentiment = ""
if 'sentiment_scores' not in st.session_state:
    st.session_state.sentiment_scores = []
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'tokenizer' not in st.session_state:
    st.session_state.tokenizer = Tokenizer()

st.info("‚ö†Ô∏è O c√≥digo ainda precisa de melhorias e ajustes. üß† Trabalhando para aprimorar todas as funcionalidades.")
st.info("üìä A funcionalidade de an√°lise de sentimento (√∫ltimo gr√°fico √† esquerda, aparece ao enviar uma mensagem) est√° operando com certa imprecis√£o e necessita de melhorias...")

with st.expander("Detalhes do Modelo"):
    st.write("""
    Este c√≥digo implementa um chatbot inteligente para an√°lise de sentimentos e detec√ß√£o de inten√ß√µes usando Streamlit. O chatbot utiliza t√©cnicas de Processamento de Linguagem Natural (PLN), incluindo:
    
    - **An√°lise de Sentimento**: Utiliza um modelo LSTM treinado com dados de sentimento (arquivo `tw_pt.csv`) para classificar o sentimento das mensagens do usu√°rio. Al√©m disso, integra o `TextBlob` para uma an√°lise complementar de polaridade.
    
    - **Detec√ß√£o de Inten√ß√µes**: Um modelo LSTM bidirecional √© treinado com embeddings do Word2Vec para detectar inten√ß√µes em mensagens de usu√°rios. O modelo √© capaz de identificar inten√ß√µes como "sauda√ß√£o", "ajuda", "despedida", entre outras, com base em um conjunto de dados do `nps_chat`.
    
    - **Pr√©-processamento de Texto**: Implementa t√©cnicas de pr√©-processamento, incluindo tokeniza√ß√£o, remo√ß√£o de stopwords, e normaliza√ß√£o de texto para garantir que as entradas sejam adequadas para os modelos.
    
    - **Tradu√ß√£o Autom√°tica**: Detecta automaticamente o idioma da mensagem do usu√°rio e a traduz para o portugu√™s, garantindo compatibilidade com os modelos treinados.
    
    - **Hist√≥rico de Conversa**: Mant√©m um hist√≥rico das intera√ß√µes do usu√°rio, permitindo respostas contextuais e uma an√°lise cont√≠nua do sentimento ao longo da conversa.
    
    - **Pipeline de Classifica√ß√£o**: Al√©m do modelo LSTM, utiliza um classificador Naive Bayes baseado em TF-IDF para an√°lise complementar de inten√ß√µes.

    - **Interface Interativa**: A interface do chatbot √© constru√≠da com Streamlit, permitindo uma intera√ß√£o fluida e amig√°vel com o usu√°rio. O hist√≥rico de conversas √© exibido em tempo real, e o usu√°rio pode limpar o hist√≥rico a qualquer momento.
    
    - **Modelo de Conversa√ß√£o**: Em casos onde a resposta gerada pelo modelo de inten√ß√£o √© muito gen√©rica, o chatbot utiliza um modelo de conversa√ß√£o baseado no DialoGPT para gerar respostas mais contextualizadas.
    
    - **Melhorias Futuras**: O c√≥digo est√° em constante evolu√ß√£o, com planos para melhorar a precis√£o da an√°lise de sentimentos, adicionar mais inten√ß√µes e integrar modelos de linguagem mais avan√ßados, como o GPT-4.
    """)

def preprocess_text(text):
    tokens = word_tokenize(text.lower())
    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]
    return " ".join(tokens)

##############################################
# MODELO DE INTEN√á√ÉO (dados do nps_chat)
##############################################

@st.cache_data
def load_intent_data():
    posts = nltk.corpus.nps_chat.xml_posts()
    data = [(post.text, post.get("class")) for post in posts]
    intent_data = pd.DataFrame(data, columns=["text", "intent"])
    intent_data['text'] = intent_data['text'].apply(preprocess_text)
    return intent_data

intent_data = load_intent_data()
X_train, X_test, y_train, y_test = train_test_split(intent_data['text'], intent_data['intent'], test_size=0.1, random_state=42)

label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

@st.cache_data
def train_word2vec(X_train):
    sentences = [text.split() for text in X_train]
    word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)
    return word2vec_model

word2vec_model = train_word2vec(X_train)

def create_embedding_matrix(word2vec_model, tokenizer, vocab_size, embedding_dim):
    embedding_matrix = np.zeros((vocab_size, embedding_dim))
    for word, i in tokenizer.word_index.items():
        if word in word2vec_model.wv:
            embedding_matrix[i] = word2vec_model.wv[word]
    return embedding_matrix

# Tokeniza√ß√£o e prepara√ß√£o dos dados para o modelo de inten√ß√£o
if 'tokenizer' not in st.session_state:
    st.session_state.tokenizer.fit_on_texts(X_train)
vocab_size = len(st.session_state.tokenizer.word_index) + 1
embedding_dim = 100  # Dimens√£o dos embeddings do Word2Vec

X_train_seq = st.session_state.tokenizer.texts_to_sequences(X_train)
X_test_seq = st.session_state.tokenizer.texts_to_sequences(X_test)

max_length = 100  # Comprimento m√°ximo das sequ√™ncias
X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')

embedding_matrix = create_embedding_matrix(word2vec_model, st.session_state.tokenizer, vocab_size, embedding_dim)

@st.cache_resource
def create_intent_model(embedding_matrix, vocab_size, embedding_dim, max_length):
    model = Sequential([
        Embedding(
            input_dim=vocab_size, 
            output_dim=embedding_dim, 
            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix), 
            input_shape=(max_length,),
            trainable=True
        ),
        Bidirectional(LSTM(128, return_sequences=True)),
        Dropout(0.3),
        Bidirectional(LSTM(64)),
        Dense(32, activation='relu'),
        Dense(len(label_encoder.classes_), activation='softmax')
    ])
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

if 'intent_model' not in st.session_state:
    st.session_state.intent_model = create_intent_model(embedding_matrix, vocab_size, embedding_dim, max_length)
    
# Callback para visualiza√ß√£o do progresso do treinamento
class ProgressBarCallback(Callback):
    def __init__(self, total_epochs):
        self.total_epochs = total_epochs
        self.progress_bar = st.progress(0)
        self.status_text = st.empty()

    def on_epoch_end(self, epoch, logs=None):
        progress = (epoch + 1) / self.total_epochs
        self.progress_bar.progress(progress)
        self.status_text.text(f"Treinamento do modelo: {epoch + 1}/{self.total_epochs} √©pocas conclu√≠das.")

# Treinamento do modelo de inten√ß√£o
if not st.session_state.model_trained:
    epochs = 10
    progress_callback = ProgressBarCallback(total_epochs=epochs)
    with st.spinner("Treinando modelo de inten√ß√£o..."):
        st.session_state.intent_model.fit(
            X_train_pad,
            y_train_encoded,
            epochs=epochs,
            batch_size=32,
            validation_split=0.1,
            callbacks=[progress_callback],
            verbose=0
        )
    if os.path.exists("modelo.weights.h5"):
        os.remove("modelo.weights.h5")
    st.session_state.model_trained = True
    st.session_state.intent_model.save_weights("modelo.weights.h5")

if os.path.exists("modelo.weights.h5"):
    st.session_state.intent_model = create_intent_model(embedding_matrix, vocab_size, embedding_dim, max_length)
    st.session_state.intent_model.load_weights("modelo.weights.h5")

# Gerar previs√µes para m√©tricas
y_pred = st.session_state.intent_model.predict(X_test_pad)
y_pred_classes = np.argmax(y_pred, axis=1)
y_pred_labels = label_encoder.inverse_transform(y_pred_classes)

##############################################
# MODELO DE SENTIMENTO (dados do tw_pt.csv)
##############################################

@st.cache_data
def load_tw_data():
    df = pd.read_csv("tw_pt.csv")
    df['Text'] = df['Text'].apply(preprocess_text)
    return df

tw_data = load_tw_data()
X_tw = tw_data['Text']
y_tw = tw_data['Classificacao']

label_encoder_tw = LabelEncoder()
y_tw_encoded = label_encoder_tw.fit_transform(y_tw)

if 'tokenizer_tw' not in st.session_state:
    st.session_state.tokenizer_tw = Tokenizer()
    st.session_state.tokenizer_tw.fit_on_texts(X_tw)
vocab_size_tw = len(st.session_state.tokenizer_tw.word_index) + 1
max_length_tw = 100
X_tw_seq = st.session_state.tokenizer_tw.texts_to_sequences(X_tw)
X_tw_pad = pad_sequences(X_tw_seq, maxlen=max_length_tw, padding='post')

def create_tw_sentiment_model(vocab_size, embedding_dim, max_length, num_classes):
    model = Sequential([
        Embedding(input_dim=vocab_size, output_dim=embedding_dim),  # Removido input_length
        LSTM(128, return_sequences=True),
        Dropout(0.3),
        LSTM(64),
        Dense(32, activation='relu'),
        Dense(num_classes, activation='softmax')
    ])
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

if 'sentiment_model_tw' not in st.session_state:
    num_classes_tw = len(label_encoder_tw.classes_)
    st.session_state.sentiment_model_tw = create_tw_sentiment_model(vocab_size_tw, embedding_dim, max_length_tw, num_classes_tw)
    epochs_tw = 5
    progress_callback_tw = ProgressBarCallback(total_epochs=epochs_tw)
    with st.spinner("Treinando modelo de sentimento..."):
        st.session_state.sentiment_model_tw.fit(
            X_tw_pad, 
            y_tw_encoded, 
            epochs=epochs_tw, 
            batch_size=32, 
            validation_split=0.1, 
            callbacks=[progress_callback_tw],
            verbose=0
        )
    st.session_state.sentiment_model_tw.save_weights("sentiment_model_tw.weights.h5")

if os.path.exists("sentiment_model_tw.weights.h5"):
    st.session_state.sentiment_model_tw = create_tw_sentiment_model(vocab_size_tw, embedding_dim, max_length_tw, len(label_encoder_tw.classes_))
    st.session_state.sentiment_model_tw.build(input_shape=(None, max_length_tw))
    st.session_state.sentiment_model_tw.load_weights("sentiment_model_tw.weights.h5")

def get_sentiment_tw(text):
    """Utiliza o modelo de sentimento treinado com tw_pt.csv para classificar o texto."""
    processed = preprocess_text(text)
    seq = st.session_state.tokenizer_tw.texts_to_sequences([processed])
    pad_seq = pad_sequences(seq, maxlen=max_length_tw, padding='post')
    pred = st.session_state.sentiment_model_tw.predict(pad_seq)
    label = label_encoder_tw.inverse_transform([np.argmax(pred, axis=1)[0]])[0]
    return label

##############################################
# CHATBOT RESPONSE
##############################################

if 'conversational_pipeline' not in st.session_state:
    try:
        # For√ßa o uso da CPU para evitar problemas com falta de GPU
        device = 0 if torch.cuda.is_available() else -1  
        st.session_state.conversational_pipeline = pipeline(
            "conversational", 
            model="microsoft/DialoGPT-small",  # Alterado para um modelo menor
            device=device  # Usa GPU se dispon√≠vel, sen√£o CPU
        )
    except Exception as e:
        st.error(f"Erro ao carregar o modelo de conversa√ß√£o: {e}")
        st.stop()

def detect_and_translate(text, target_lang='pt'):
    try:
        detected_lang = detect(text)
    except Exception as e:
        detected_lang = target_lang
    translated_text = GoogleTranslator(source=detected_lang, target=target_lang).translate(text) if detected_lang != target_lang else text
    return translated_text

if 'knowledge_base' not in st.session_state:
    st.session_state.knowledge_base = {}  # Ex: {"brasil": "Am√©rica do Sul"}

def similar(a, b):
    """Retorna a similaridade entre duas strings (0 a 1)."""
    return SequenceMatcher(None, a, b).ratio()

def atualizar_knowledge_base(texto):
    #regex para capturar afirma√ß√µes do tipo "O Brasil est√° localizado na Am√©rica do Sul"
    padrao = r"(\w[\w\s]+)\s+est[a√°] localizado(?:a)?\s+(?:na|no|em)\s+([\w\s]+)"
    correspondencia = re.search(padrao, texto, re.IGNORECASE)
    if correspondencia:
        entidade = correspondencia.group(1).strip().lower()
        localizacao = correspondencia.group(2).strip()
        st.session_state.knowledge_base[entidade] = localizacao
        # Opcional: exibir no log ou dar feedback ao usu√°rio
        # print(f"Atualizando conhecimento: {entidade} -> {localizacao}")

def buscar_resposta_no_knowledge_base(texto):
    # Extrair poss√≠veis entidades 
    palavras = texto.split()
    for chave, localizacao in st.session_state.knowledge_base.items():
        for palavra in palavras:
            if similar(chave, palavra.lower()) > 0.8:
                return f"{chave.title()} est√° localizado(a) em {localizacao}."
    return None

def chatbot_response(user_input):

    translated_input = detect_and_translate(user_input)
    
    #Atualiza o banco de conhecimento se o usu√°rio estiver fornecendo um fato
    atualizar_knowledge_base(translated_input)
    
    blob = TextBlob(translated_input)
    polarity = blob.sentiment.polarity  # valor entre -1 (muito negativo) e 1 (muito positivo)
    if polarity < -0.2:
        sentiment_label = "negativo"
    elif polarity > 0.2:
        sentiment_label = "positivo"
    else:
        sentiment_label = "neutro"
    
    #Pr√©-processamento para previs√£o de inten√ß√£o 
    processed_input = preprocess_text(translated_input)
    input_seq = st.session_state.tokenizer.texts_to_sequences([processed_input])
    input_pad = pad_sequences(input_seq, maxlen=max_length, padding='post')
    
    with st.spinner("Processando inten√ß√£o..."):
        intent_pred = st.session_state.intent_model.predict(input_pad)
    intent_class = np.argmax(intent_pred, axis=1)
    intent_label = label_encoder.inverse_transform(intent_class)[0]
    
    # inten√ß√£o com regras baseadas em palavras-chave
    input_lower = translated_input.lower()
    if any(greet in input_lower for greet in ["ol√°", "oi", "bom dia", "boa tarde", "boa noite"]):
        intent_label = "sauda√ß√£o"
    elif "nome" in input_lower:
        intent_label = "nome"
    elif any(ajuda in input_lower for ajuda in ["ajuda", "suporte", "preciso de ajuda"]):
        intent_label = "ajuda"
    elif any(cumprimento in input_lower for cumprimento in ["tudo bem", "como vai", "como est√°"]):
        intent_label = "sauda√ß√£o"
    
    # Templates de resposta para cada inten√ß√£o
    response_templates = {
        "sauda√ß√£o": [
            "Ol√°! Como posso ajudar?",
            "Oi! Tudo bem?",
            "Bom dia! Em que posso ser √∫til?",
            "Oi! Como vai voc√™?"
        ],
        "ajuda": [
            "Claro! Qual sua d√∫vida?",
            "Estou aqui para ajudar. Como posso ser √∫til?",
            "Diga-me como posso ajudar voc√™!"
        ],
        "documento": [
            "Envie o documento para an√°lise.",
            "Vou processar seu documento. Por favor, envie-o.",
            "Por favor, anexe o documento que deseja analisar."
        ],
        "raiva": [
            "Sinto muito que voc√™ esteja frustrado. O que posso fazer para ajudar?",
            "Entendo sua raiva. Vamos ver como resolver isso juntos.",
            "Pe√ßo desculpas se algo n√£o saiu bem. Conte-me o que aconteceu."
        ],
        "elogio": [
            "Muito obrigado! Fico feliz em ajudar.",
            "Agrade√ßo pelo feedback positivo!",
            "√â √≥timo saber que voc√™ est√° satisfeito."
        ],
        "tristeza": [
            "Sinto muito que voc√™ esteja se sentindo assim. Quer conversar sobre isso?",
            "Lamento ouvir isso. Estou aqui se precisar desabafar.",
            "Sei que momentos dif√≠ceis acontecem. Estou aqui para ajudar no que for poss√≠vel."
        ],
        "nome": [
            "Meu nome √© Chatbot. Em que posso ajudar hoje?",
            "Eu sou o Chatbot, seu assistente virtual.",
            "Pode me chamar de Chatbot. Como posso ajudar?"
        ],
        "felicidade": [
            "Que bom que voc√™ est√° feliz! Em que posso ajudar?",
            "Fico contente em saber que voc√™ est√° bem!",
            "Sua felicidade me alegra! O que posso fazer por voc√™?"
        ],
        "despedida": [
            "At√© logo! Espero ter ajudado.",
            "Tchau! Volte sempre que precisar.",
            "At√© mais! Estarei aqui se precisar de algo."
        ],
        "d√∫vida": [
            "Pode me explicar melhor?",
            "N√£o entendi completamente. Poderia reformular?",
            "Poderia dar mais detalhes para que eu possa ajudar?"
        ]
    }
    
    response = None

    # Verifica se a pergunta pode ser respondida usando o conhecimento previamente armazenado
    knowledge_response = buscar_resposta_no_knowledge_base(translated_input)
    if knowledge_response:
        response = knowledge_response
        # Atualiza o hist√≥rico
        st.session_state.chat_history.append(("Voc√™", translated_input))
        st.session_state.chat_history.append(("Chatbot", response))
        return response, sentiment_label

    # Se n√£o houver correspond√™ncia no banco de conhecimento, decide entre usar templates ou a pipeline conversacional
    if not st.session_state.chat_history or intent_label not in response_templates:
        with st.spinner("Gerando resposta..."):
            st.session_state.chat_history.append(("Voc√™", translated_input))
            # Limita o hist√≥rico para evitar excesso de informa√ß√µes irrelevantes
            contexto = st.session_state.chat_history[-6:]  # √∫ltimos 3 turnos (voc√™ e chatbot)
            conversation_text = "\n".join([f"{sender}: {msg}" for sender, msg in contexto])
            conversation = Conversation(conversation_text)
            result = st.session_state.conversational_pipeline(conversation)
            generated_response = result.generated_responses[-1]
            # P√≥s-processamento simples para remover repeti√ß√µes indesejadas
            generated_response = re.sub(r'(Voc eu\s?)+', 'Voc√™', generated_response, flags=re.IGNORECASE)
            st.session_state.chat_history.append(("Chatbot", generated_response))
            response = generated_response
    else:
        # Se houver template, seleciona uma resposta de acordo com a inten√ß√£o e o sentimento
        if sentiment_label == "negativo" or intent_label in ["raiva", "tristeza"]:
            response = random.choice(response_templates["tristeza"])
        elif sentiment_label == "positivo" and intent_label == "elogio":
            response = random.choice(response_templates.get("felicidade", response_templates["elogio"]))
        else:
            response = random.choice(response_templates.get(intent_label, ["Desculpe, n√£o entendi."]))
        st.session_state.chat_history.append(("Voc√™", translated_input))
        st.session_state.chat_history.append(("Chatbot", response))
    
    return response, sentiment_label
##############################################
# INTERFACE DO CHAT
##############################################

chat_container = st.container()

with chat_container:
    chat_placeholder = st.empty()
    
    # Renderizando mensagens anteriores
    for sender, message in st.session_state.chat_history:
        if sender == "Voc√™":
            st.chat_message("user").markdown(f"<strong>Voc√™:</strong> {message}", unsafe_allow_html=True)
        else:
            with st.chat_message("assistant"):
                st.markdown(f"<strong>Chatbot:</strong> {message}", unsafe_allow_html=True)

user_input = st.chat_input("Digite sua mensagem")
if user_input:
    with st.spinner("Processando sua mensagem..."):
        response, sentiment = chatbot_response(user_input)
        
    # Adicionando a mensagem do usu√°rio e a resposta do chatbot ao hist√≥rico
    st.session_state.chat_history.append(("Voc√™", user_input))
    st.session_state.chat_history.append(("Chatbot", response))
    st.session_state.last_sentiment = sentiment
    
    # Re-renderizando as mensagens
    for sender, message in st.session_state.chat_history:
        if sender == "Voc√™":
            st.chat_message("user").markdown(f"<strong>Voc√™:</strong> {message}", unsafe_allow_html=True)
        else:
            with st.chat_message("assistant"):
                st.markdown(f"<strong>Chatbot:</strong> {message}", unsafe_allow_html=True)
    
 
    st.rerun()

##############################################
# SIDEBAR COM M√âTRICAS E AN√ÅLISES
##############################################

st.sidebar.header("An√°lises e M√©tricas")
st.sidebar.subheader("Desempenho do Modelo de Inten√ß√£o")
st.sidebar.write("Accuracy:", accuracy_score(y_test, y_pred_labels))
st.sidebar.write("Classification Report:")
st.sidebar.text(classification_report(y_test, y_pred_labels))

st.sidebar.subheader("Sentimento da √öltima Mensagem")
st.sidebar.write(f"{st.session_state.last_sentiment}")

st.sidebar.subheader("Evolu√ß√£o do Sentimento")
# Extrai os sentimentos das mensagens do usu√°rio utilizando o modelo tw_pt
sentiments = [get_sentiment_tw(msg[1]) for msg in st.session_state.chat_history if msg[0] == "Voc√™"]
if sentiments:
    # Converte os r√≥tulos para pontua√ß√µes para plot (ex.: Positivo: 1, Neutro: 0, Negativo: -1)
    sentiment_scores = [1 if s.lower() in ["positivo", "felicidade", "elogio"] else -1 if s.lower() in ["negativo", "raiva", "triste"] else 0 for s in sentiments]
    fig, ax = plt.subplots()
    sns.lineplot(x=range(len(sentiment_scores)), y=sentiment_scores, ax=ax, marker='o')
    ax.set_title("Evolu√ß√£o do Sentimento")
    ax.set_xlabel("Mensagens")
    ax.set_ylabel("Pontua√ß√£o de Sentimento")
    st.sidebar.pyplot(fig)

if st.sidebar.button("Limpar Hist√≥rico do Chat"):
    st.session_state.chat_history = []
    st.rerun()
